<!-- Copyright 2019 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. -->

<html>

<head>
  <link rel="icon"
        type="image/png"
        href="./icon.png">
  <link rel="stylesheet"
        href="index.css">
  <meta charset="utf-8">
  <meta name="viewport"
        content="width=device-width, initial-scale=1">
  <meta property="og:title"
        content="Depth in Art History Visualization">
  <meta property="og:image"
        content="header.png">
  <meta name="twitter:card"
        content="summary_large_image">

  <title>Depth Predictions in Art</title>
</head>

<body>
  <div
       style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;">
    <div id="MathJax_Hidden"></div>
  </div>
  <div id="MathJax_Message"
       style="display: none;"></div>
  <div id="header"
       style="height: 100px;">
    <img src='images/header_img.png'>
    <div>
      <h1 style="margin-top:20px; margin-bottom:20px">Depth Predictions in Art
      </h1>
    </div>

    <div style="color:#999;font-style:italic">
      By Ellen Jiang, Emily Reif, and Been Kim on <a target='blank'
         href='https://ai.google/research/teams/brain/pair'>PAIR</a>
      <br>
      This post accompanies our <a target="blank"
         href="https://github.com/PAIR-code/depth-maps-art-and-illusions">open-sourced</a>
      <a target="blank"
         href="go/depth-in-art">visualization</a>.
    </div>

    Models for depth prediction are usually trained on realistic photo/video data. However,
    exploring
    how models perform on artwork also produces some interesting results. We found that such models
    can perform surprisingly well on these images. In our project, we qualitatively explore the
    depth
    interpretations of a database of art history images, looked at how models perform on optical
    illusions, and experimented with generating adversarial examples.
    <p>
      Painters throughout art history have used various techniques to represent our
      three-dimensional
      world on a two-dimensional canvas. Through deliberate use of linear and atmospheric
      perspective,
      hard and soft edges, overlay of shapes, and nuanced hue and saturation, painters can rendered
      convincing illusions of depth on flat surfaces. These painted images, with varying degrees of
      “depth illusion" can also be interpreted by something entirely different: machine learning
      models.
    </p>
    <p>
      Models for depth prediction are usually trained on realistic photo/video data. However,
      exploring
      how models perform on artwork also produces some interesting results. We found that such
      models
      can perform surprisingly well on these images. In our project, we qualitatively explore the
      depth
      interpretations of a database of art history images, looked at how models perform on optical
      illusions, and experimented with generating adversarial examples.
    </p>
    <p>
      We’ve created an <a target="blank"
         href="https://github.com/PAIR-code/depth-maps-art-and-illusions">open-source</a>,
      <a target="blank"
         href="http://ellenj.cam.corp.google.com:1234/">interactive visualization</a>
      to explore many of these depth
      interpretations.
    </p>

    <h2>Depth Prediction Models</h2>
    <p>
      From an RGB image input, the depth prediction model we use chooses a scalar ‘depth’ value for
      each pixel somewhere between black and white, where lighter pixels are understood as closer
      and
      darker pixels as farther away.
    </p>
    <div class='img-holder'>
      <img class='small'
           src='images/interior.png'>
      <img class='small'
           src='images/interior_output.png'>
    </div>
    <div class='caption'>
      Here, the tables are interpreted as receding into space (<a target="blank"
         href="https://ccsearch.creativecommons.org/photos/0bcee165-8b4f-4286-8690-150fb009deda">source</a>)
    </div>

    <h2>Art History Dataset</h2>
    <p>
      Our dataset is 63,000 paintings from collections from the MET and the Rijksmuseum, with
      paintings from infrequent art styles filtered out. Unfortunately, due to a skewed dataset,
      this
      resulted in mostly Western styles. A future research direction is to analyze other styles more
      deeply; the treatment of depth in art varies widely through different locations and eras, and
      having that data would give us a much richer exploration. For example, ancient Chinese artwork
      often employs isometric perspective, a totally different system of portraying depth than the
      Renaissance style of linear perspective. How would this change the output of the model?
      However,
      for now, our exploration was limited to artworks in the filtered styles.
    </p>
    <p>
      We ran this dataset through a deep learning model that outputs a predicted depth map. This
      model
      learned to interpret 3D shape by observing tens of thousands of scenes shot with a moving
      camera, as in
      <a target="blank"
         href="https://ai.google/research/pubs/pub46965">prior Google research</a>.
    </p>
    <div class='img-holder'>
      <img class='small'
           src='images/art_history_example_input.png'>
      <img class='small'
           src='images/art_history_example_output.png'>
    </div>
    <div class='caption'>The Miraculous Draft of Fishes, Raphael, 1515 (
      <a target="blank"
         href="https://artsandculture.google.com/asset/the-miraculous-draft-of-fishes/ywH0tF2CcmYUxA">source</a>)
    </div>
    <h2>Visualization</h2>
    <p>
      We then created an
      <a target="blank"
         href="http://ellenj.cam.corp.google.com:1234/">interactive visualization</a> that allows
      for
      exploration of these paintings and their depth
      maps over time, displaying examples in an interactive point cloud projection.
    </p>
    <h3>Point Cloud Projection</h3>
    <p>
      It can be hard to tell which parts of the image are in front of others in a more detailed
      image
      with finer value gradations in the depth map. It isn’t always easy to imagine what a depth map
      looks like in three dimensions.
    </p>
    <p>
      We created a 3D-view of the original image using the depth-map prediction values, by
      projecting
      each pixel into the third dimension with its depth value as its z-position. The
      three-dimensional view of the depth prediction revealed many errors that are harder to pick up
      in just a flat depth map, and provides a more intuitive interface for gauging the success of a
      prediction.
    </p>
    <div class='img-holder'>
      <img class='img-small'
           src='images/building orig pic.png'>
      <img class='img-small'
           src='images/building depth.png'>
    </div>
    <div class='caption'>Print, Utagawa Toyoharuca, 1780 (
      <a target="blank"
         href="https://artsandculture.google.com/asset/zQFBPADRZqeuUA">source</a>)</div>
    <img class='img-small'
         src='images/buildings_pointcloud.gif'>
    <div class='caption'>Pointcloud depth map visualization</div>
    <p>The three-dimensional view of the depth prediction revealed many errors that are harder to
      pick
      up in just a flat depth map, and provides a more intuitive interface for gauging the success
      of
      a prediction.</p>
    <h3>Graph Mode</h3>
    <p>
      The depth figure in the web app’s Graph Mode plots paintings by their year and depth range,
      color-coded by their art movement. Pan and zoom on the plot in the app and select points to
      see
      their paintings in the viewer.
    </p>
    <img src='images/visualization_graph_view.png'>
    <div class='caption'>Graph view</div>

    <h3>Image Mode</h3>
    <p>
      Image mode displays a list of paintings chronologically. Paintings can be selected for display
      in the viewer.
    </p>
    <img src='images/visualization_image_view.png'>
    <div class='caption'>Image view</div>

    <h2>Observations</h2>
    <p>
      In addition to exploring the overall depth trends through time and by style, the visualization
      can also be used for finding edge cases and error cases of the model used to make the
      predictions.
    </p>
    <p>
      It should be noted that different models will exhibit different kinds of failure modes,
      depending on a few things including their training data, architecture, and assumptions
      underlying the training. For instance, the data used to train this specific model often had
      depth edges coincident with color edges, which may contribute to some of the observations
      below;
      other depth predictions models might not exhibit such issues, but may exhibit others.
    </p>
    <p>
      Also, as mentioned above, art is entirely outside of the training distribution, so it’s not
      surprising that the model sometimes struggles with it (indeed, it’s impressive how well the
      model does extrapolate to this unseen data). However, seeing overall trends with certain types
      of images, styles, and features can hint at the underlying prediction mechanism.
    </p>
    <h3>Frames</h3>
    <p>
      Frames and borders sometimes seem to cause the depth map to flatten. Of course, this is in
      some
      ways by design-- the model might be using the frame as a clue that the image is really an
      image
      of a flat painting, rather than a 3D scene. Still, it’s an interesting question of why the
      frame
      flattens the image, rather than acting as a window into the scene.
    </p>
    <p>
      Incidentally, the idea that frames distinguish art from the rest of the world is studied in
      the
      art world as well. In
      <a target="blank"
         href="https://pdfs.semanticscholar.org/3a9f/7dd7e687c99a7f7fc4c47c08b4789b0721ea.pdf">this
        essay</a>, Ke-bing Tang argues that frames serve as a “boundary mark of
      identifying the art and reality.” Similarly,
      <a target="blank"
         href="https://www.amazon.com/Rhetoric-Frame-Boundaries-Cambridge-Criticism/dp/0521566290">Paul
        Duro </a>
      says that with the frame, “we experience
      the illusory coherence of the artwork,” and that it is “a marker of limits.”
    </p>
    <p>
      Another note about frames is that they often aren’t interpreted as being consistently on the
      same plane, with changes in depth along the border. It would be interesting to explore this
      further.
    </p>
    <div class='img-holder'>
      <img class='img-small'
           src='images/pheasant_with_frame_input.png'>
      <img class='img-small'
           src='images/pheasant_with_frame_output.png'>
      <img class='img-small'
           src='images/pheasant_no_frame_input.png'>
      <img class='img-small'
           src='images/pheasant_no_frame_output.png'>
    </div>
    <div class='caption'>Pheasant Shooting, by Henry Thomas Alken, 1820 (<a target="blank"
         href="https://artsandculture.google.com/asset/pheasant-shooting/fgG_sfeG64eMwA">source</a>)
    </div>
    <!-- TODO: add attribution -->
    <div class='img-holder'>
      <img class='img-small'
           src='images/winged_with_frame_input.png'>
      <img class='img-small'
           src='images/winged_with_frame_output.png'>
      <img class='img-small'
           src='images/winged_no_frame_input.png'>
      <img class='img-small'
           src='images/winged_no_frame_output.png'>
    </div>
    <div class='caption'>Winged Figure Seated Upon a Rock, by Abbott Handerson Thayer, 1914 (<a
         target="blank"
         href="https://artsandculture.google.com/asset/winged-figure-seated-upon-a-rock/BwHUF8iHPTRljQ">source</a>)
    </div>
    <h3>Faces</h3>
    <p>
      In portraiture, faces are often painted to be more dimensional and are brightly lit to stand
      out
      as a focal point, with a flatter, more muted rendering of surrounding areas. We found that in
      depth predictions for portraits, the face is often sharply brighter than the background and
      juts
      out of the canvas.
    </p>
    <p>
      Based on the skew toward Western art traditions in this data set, we additionally checked
      whether the effect seemed to hold for a wider variety of skin types. Indeed, as seen below,
      this
      seems to be the case.
    </p>
    <div class='img-holder'>
      <img class='img-small'
           src='images/portrait.png'>
      <img class='img-small'
           src='images/portrait_depth.png'>
    </div>
    <div class='caption'>My Little Brother, Jong Gu Lee, 1985
      <a target="blank"
         href="https://artsandculture.google.com/asset/JwH8qA4j8_Ji4w">source</a>)</div>

    <div class='img-holder'>
      <img class='img-small'
           src='images/portrait_2.png'>
      <img class='img-small'
           src='images/portrait_2_depth.png'>
    </div>
    <div class='caption'>Self-Portrait, Joshua Reynolds, 1745/1749
      <a target="blank"
         href="https://artsandculture.google.com/asset/_wHDJZ0TGlEIWw">source</a>)</div>

    <div class='img-holder'>
      <img class='img-small'
           src='images/baron.png'>
      <img class='img-small'
           src='images/baron depth.png'>
    </div>
    <div class='caption'>John, 1st Baron Byron, Kehinde Wiley, 2013, (<a target="blank"
         href="https://artsandculture.google.com/exhibit/MwLCMuU4yTWnLA">source</a>)</div>
    <h3>Effect of flipping images</h3>
    <p>
      The model outputs flat depth maps for some images and detailed depth maps for their
      horizontally
      reflected counterparts.
    </p>
    <h3>Looking at Depth across Different Styles
    </h3>
    <p>
      A natural question is whether there are trends in the model across time and styles. In the
      Western tradition, many of the depth techniques described earlier (namely linear perspective)
      were matured around the time of the Renaissance in the fifteenth and sixteenth centuries.
      Then,
      starting around the end of the nineteenth century, many Modern Art movements began to break
      from
      this tradition of portraying illusionistic depth in representational images and began to
      abstract their subjects. In addition, while the classical Western art tends to pursue
      illusionistic realism, some non-western traditions of painting have long gravitated towards
      flatter colors and more stylized representations of nature.
    </p>
    <p>
      So, we initially expected there might be an increasing trend around 1500, and a decreasing
      trend
      in the 1900s due to the development of perspective and the later modern art movements that
      flattened illusionistic paintings.
    </p>
    <p>
      But how do you measure this? There are various metrics to approximate the overall depth of the
      image given by the model, that is, a scalar depth value for the image from the pixels of the
      depth map. We tried median, range (maximum depth - minimum depth), and standard deviation.
    </p>
    <p>
      We then calculated the mean and standard deviation of the distribution of these metrics for
      two
      groups of paintings:
    </p>
    <ul>
      <li>
        <b>Realistic / naturalistic:</b> Academic art, American Realism, Realism, Neoclassicism
      </li>
      <li>
        <b>Non-realistic / abstract:</b> Non-realistic / abstract: Street art, Abstract art,
        Geometric
        abstraction, Shin-hanga, American modernism, Ukiyo-e
      </li>
    </ul>
    <p>
      Surprisingly, we found that there isn’t much statistical significance for these depth metrics
      across the two style groups.
    </p>
    <img src='images/boxplot.png'>
    <p>
      Similarly, plotting a number of art movements’ aggregate depth values over time, we found they
      also fall within a similar range. Note that the error bars are for standard deviation in the y
      direction, and year in the x direction, for a number of art movements,
    </p>
    <img src='images/art_styles_error_bars.png'>
    <p>
      The fact that depth doesn’t seem to be correlated with either time or artistic style raises an
      interesting question. Which is wrong, our interpretation of depth in art history, or the
      model’s
      interpretations of the images?
    </p>
    <p>
      Of course, all these metrics have a potential flaw: perhaps some depth image outputs are not
      just flat or deep, but actually incorrect. This, of course, is impossible to calculate as we
      have no ground truth of depth for these images. A future direction could be to try and find an
      uncertainty measure for the depth, and if this correlates more with art movements.
    </p>

    <h2>Other Out-of-Distribution Images</h2>
    <h3>Optical Illusions</h3>
    <div class='img-holder'>
      <img class='img-small'
           src='images/escher_waterfall.jpg'>
      <img class='img-small'
           src='images/waterfall_depth.png'>
    </div>
    <div class='caption'>Waterfall, Escher, 1961 (<a target="blank"
         href="https://www.mcescher.com/gallery/recognition-success/waterfall/">source</a>)</div>
    <p>
      In the context of depth prediction, <a target="blank"
         href="https://en.wikipedia.org/wiki/Impossible_object">impossible object illusions</a>
      are another set of interesting
      image examples. For example, in the image above, the water is simultaneously flowing away from
      the
      wheel along the trough, but also back toward the waterfall, which is directly above the wheel
      (there are also a few other hidden impossibilities in the picture).
    </p>
    <p>
      These images purposefully depict physically impossible objects or spaces, so no true depth map
      can exist. How, then, will the model react when it is forced to make a depth prediction? Will
      it
      look at local edges or try to glean some global structure from the image? It’s hard to
      quantitatively analyze this or make a generalizable conclusion, but from this example of an
      Escher print, the model does seem to be trying to impose some sort of global structure on the
      image. That is, the water trough does seem to be interpreted as a single cohesive element
      which
      uniformly moves into the background, rather than being bent to satisfy the constraint of the
      towers.
    </p>


    <h3>Generating Adversarial Examples</h3>
    <p>
      The out-of-model-distribution artworks described in the previous section can act as
      adversarial
      examples to explore the model’s capabilities. What about generated adversarial examples, in
      the
      machine learning sense?
    </p>
    <p>
      We also played with with generating adversarial examples for depth models using the standard
      gradient descent approach (see generation script <a target="blank"
         href="https://github.com/PAIR-code/depth-maps-art-and-illusions/tree/master/adversarial">here</a>).
      These experiments were on this
      <a target="blank"
         href="https://github.com/ialhashim/DenseDepth">open-source DenseDepth model</a>.
    </p>
    <p>
      In one example, we started with this image of a bathroom sink (left), which produces the
      corresponding depth map (middle), and we tried to manipulate the input image so that the
      output
      depth map was that of the bathtub (right).
    </p>
    <div class='img-holder'>
      <img class='img-small'
           src='images/adversarial_original_input.png'>
      <img class='img-small'
           src='images/adversarial_original_output.png'>
      <img class='img-small'
           src='images/adversarial_target_output.png'>
    </div>
    <div class='captions-holder'>
      <div class='caption'>Original image</div>
      <div class='caption'>Depth map of original image</div>
      <div class='caption'>Target depth map</div>
    </div>
    <p>
      We ran gradient descent, altering the image input to with the loss as the mean-squared error
      of
      the difference between the generated depth output and the target depth output. After 250
      iterations with a step size of .001, we generated the following:
    </p>
    <div class='img-holder'>
      <img class='img-small'
           src='images/adversarial_original_input.png'>
      <img class='img-small'
           src='images/adversarial_generated_input.png'>
      <img class='img-small'
           src='images/adversarial_generated_output.png'>
    </div>
    <div class='captions-holder'>
      <div class='caption'>Original image</div>
      <div class='caption'>Manpulated image</div>
      <div class='caption'>Depth map of manipulated image</div>
    </div>

    <h2>Conclusion</h2>
    <p>
      As with many machine learning models, depth prediction models can be hard to accurately
      evaluate
      and debug. However, there has been much less work on interpreting their failures than there
      has
      been on other image models. This raises some interesting questions. When they fail, how do
      they
      fail? What kind of adversarial examples can we generate to force them to fail? And how,
      overall,
      are they making predictions?
    </p>
    <p>
      One lens through which to approach this is art history: humans have been attempting to define
      and control two dimensional depth representations for millennia, and perhaps we can take
      advantage of this. By observing how the model performs on images that are out of its training
      distribution in some ways (but not all), we can begin to isolate and explore what factors the
      model may be focusing on to make its prediction. We hope that this visualization interface in
      dataset can be helpful to other researchers in qualitatively exploring their models as well.
    </p>

    <h2>Thanks!</h2>
    <div style="color:#999;font-style:italic;padding-bottom:30px;">
      Thanks to Martin Wattenberg, Fernanda Viegas, Noah Snavely, Richard
      Tucker, D. Sculley, Damien Henry, and Jasper Snoek for their help, feedback, and editing
    </div>
</body>

</html>

<!-- Copyright 2019 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. -->

<html>

<head>
  <link rel="icon"
        type="image/png"
        href="./icon.png">
  <link rel="stylesheet"
        href="index.css">
  <meta charset="utf-8">
  <meta name="viewport"
        content="width=device-width, initial-scale=1">
  <meta property="og:title"
        content="Depth in Art History Visualization">
  <meta property="og:image"
        content="header.png">
  <meta name="twitter:card"
        content="summary_large_image">

  <title>Depth Predictions in Art</title>
</head>

<body>
  <div
       style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;">
    <div id="MathJax_Hidden"></div>
  </div>
  <div id="MathJax_Message"
       style="display: none;"></div>
  <div id="header"
       style="height: 100px;">
    <img src='images/header_img.png'>
    <div>
      <h1 style="margin-top:20px; margin-bottom:20px">Depth Predictions in Art
      </h1>
    </div>

    <div style="color:#999;font-style:italic">
      By Ellen Jiang, Emily Reif, and Been Kim on <a target='blank'
         href='https://ai.google/research/teams/brain/pair'>PAIR</a>
      <br>
      <br>
      This is the first of two posts (second is <a href='./blogpost_2.html'
         target='_blank'>here</a>) that accompany our
      <a target="blank"
         href="https://storage.googleapis.com/art_history_depth_data/demo/index.html">visualization</a>,
      which is
      <a target="blank"
         href="https://github.com/PAIR-code/depth-maps-art-and-illusions">open-sourced</a>.
    </div>
    <p>

      Painters throughout art history have used various techniques to represent our
      three-dimensional world on a two-dimensional canvas. Using linear and atmospheric perspective,
      hard and soft edges, overlay of shapes, and nuanced hue and saturation, painters can render
      convincing illusions of depth on flat surfaces. These painted images, with varying degrees of
      “depth illusion" can also be interpreted by something entirely different: machine learning
      models.
    </p>

    <p>
      Models for depth prediction are usually trained on realistic photo or video data. Exploring
      how they perform on artworks (i.e., image explicitly intended to have varying levels of depth
      using different techniques) is a way to better understand how our depth perception is similar
      or different from theirs. In this investigation, we qualitatively explore the depth
      interpretations of a database of art history images. Interestingly, we found that these models
      perform surprisingly well on such images, despite the fact that the differed so much from the
      training data.
    </p>

    <p>
      We’ve created an <a target="blank"
         href="https://github.com/PAIR-code/depth-maps-art-and-illusions">open-source</a>,
      <a target="blank"
         href="https://storage.googleapis.com/art_history_depth_data/demo/index.html">interactive
        visualization</a>
      to explore many of these depth
      interpretations.
    </p>

    <h2>Depth Prediction Models</h2>
    <p>
      From an RGB image input, a depth prediction model chooses a scalar ‘depth’ value for each
      pixel. These values are then normalized between 0 and 255, and visualized as a grayscale
      images where lighter pixels are understood as closer and darker pixels as farther away. The
      model we used learned to interpret 3D shapes by observing tens of thousands of scenes shot
      with a moving camera, an approach that has been used in <a
         href='https://ai.google/research/pubs/pub46965'
         target='_blank'>Google research.</a>
    </p>
    <div class='img-holder'>
      <img class='small'
           src='images/interior.png'>
      <img class='small'
           src='images/interior_output.png'>
    </div>
    <div class='caption'>
      Here, the tables are interpreted as receding into space (<a target="blank"
         href="https://ccsearch.creativecommons.org/photos/0bcee165-8b4f-4286-8690-150fb009deda">source</a>)
    </div>

    <h2>Art History Dataset</h2>
    <p>
      Our dataset is 63,000 paintings from collections from the Metropolitan Museum of Art and the
      Rijksmuseum in Amsterdam, hosted by Google Arts and Culture
    </p>
    <div class='side-note'>
      From this dataset, we filtered out paintings from infrequent art styles. Unfortunately, due to
      a skewed dataset, this resulted in taking out most non-Western styles. A future research
      direction would be to analyze other styles more deeply as the treatment of depth in art varies
      widely through different locations and eras. For example, ancient Chinese artwork often
      employs isometric perspective, a totally different system of portraying depth than the
      Renaissance style of linear perspective. How would this change the output of the model?
      However, for now, our exploration was limited to artworks in the filtered styles.
    </div>
    <p>
      We ran this dataset through the model described above to get a depth map for each image.
    </p>
    <div class='img-holder'>
      <img class='small'
           src='images/art_history_example_input.png'>
      <img class='small'
           src='images/art_history_example_output.png'>
    </div>
    <div class='caption'>The Miraculous Draft of Fishes, Raphael, 1515 (
      <a target="blank"
         href="https://artsandculture.google.com/asset/the-miraculous-draft-of-fishes/ywH0tF2CcmYUxA">source</a>)
    </div>
    <h2>Visualization</h2>
    <p>
      We created an <a target="blank"
         href="https://storage.googleapis.com/art_history_depth_data/demo/index.html">interactive
        visualization</a> that allows for exploration of these paintings and
      their depth maps over time, displaying the selected example in an interactive point cloud
      projection.
    </p>
    <h3>Graph Mode</h3>
    <p>
      The plot in the web app’s Graph Mode shows paintings by the year when they were first
      exhibited, and depth range. There are various metrics to approximate the overall depth of the
      image given by the model, that is, a scalar depth value for the image from the pixels of the
      depth map. We explore some of these further later in this blogpost, and here use range
      (maximum depth - minimum depth).
      In the depth plot, the images are color-coded by the art movement they belong to. Pan and zoom
      on the plot in the app and select points to see their paintings in the viewer.
    </p>
    <img src='images/graph.gif'>
    <div class='caption'>Graph view</div>

    <h3>Image Mode</h3>
    <p>
      Image mode displays a list of paintings which can be selected for display in the viewer.
    </p>
    <img src='images/images.gif'>
    <div class='caption'>Image view</div>

    <h3>Point Cloud Projection</h3>
    <p>
      It can be hard to tell which parts of the image are in front of others in a more detailed
      image with finer value gradations in the depth map.
    </p>
    <p>
      We created a 3D-view of the original image using the depth-map prediction values, by
      projecting each pixel into the third dimension with its depth value as its z-position. The
      three-dimensional view of the depth prediction revealed many errors that are harder to pick up
      in just a flat depth map, and provides a more intuitive interface for gauging the success of a
      prediction.

    </p>
    <div class='img-holder'>
      <img src='images/building orig pic.png'>
      <img src='images/building depth.png'>
    </div>
    <div class='caption'>Print, Utagawa Toyoharuca, 1780 (
      <a target="blank"
         href="https://artsandculture.google.com/asset/zQFBPADRZqeuUA">source</a>)</div>
    <img src='images/buildings_pointcloud.gif'>
    <div class='caption'>Pointcloud depth map visualization</div>

    <div class='img-holder'>
      <img src='images/arch_img.png'>
      <img src='images/arch_depth.png'>
    </div>
    <div class='caption'>Interior of the Church of St Bavo in Haarlem, Pieter Jansz. Saenredam,
      1636/1636(
      <a target="blank"
         href="https://artsandculture.google.com/asset/2wHYnTzq5M0zLg">source</a>)</div>
    <img src='images/arch_depth.gif'>
    <div class='caption'>Pointcloud depth map visualization</div>


    <p>
      In this case, the depth map appears normal at first glance. However, on inspecting the
      pointcloud, we see that there are in fact some major problems, and that the arch itself is
      not even on the same plane.
    </p>
    <h2>Observations</h2>
    <p>
      In addition to exploring the overall depth trends through time and by style, the visualization
      can also be used for finding edge cases and error instances of the model used to make the
      predictions.
    </p>
    <p>
      It should be noted that different models will exhibit different kinds of failure modes,
      depending on things such as training data, architecture, and assumptions underlying the
      training. For instance, the data used to train this specific model often had depth edges
      coincident with color edges, which may contribute to some of the observations below; other
      depth
      predictions models might not exhibit such issues, but may exhibit others.
    </p>
    <p>
      Also, as mentioned above, art is entirely outside of the training distribution, so it’s not
      surprising that the model sometimes struggles with it (indeed, it’s impressive how well the
      model extrapolates to this unseen data). However, seeing overall trends with certain types of
      images, styles, and features can hint at the underlying prediction mechanism.
    </p>
    <h3>Frames</h3>
    <p>
      Frames and borders sometimes seem to cause the depth map to flatten. Of course, this is in
      some
      ways by design-- the model might be using the frame as a clue that the image is really an
      image
      of a flat painting, rather than a 3D scene. Still, it’s an interesting question of why the
      frame
      flattens the image, rather than acting as a window into the scene.
    </p>
    <p>
      Incidentally, the idea that frames distinguish art from the rest of the world is studied in
      the
      art world as well. In
      <a target="blank"
         href="https://pdfs.semanticscholar.org/3a9f/7dd7e687c99a7f7fc4c47c08b4789b0721ea.pdf">this
        essay</a>, Ke-bing Tang argues that frames serve as a “boundary mark of
      identifying the art and reality.” Similarly,
      <a target="blank"
         href="https://www.amazon.com/Rhetoric-Frame-Boundaries-Cambridge-Criticism/dp/0521566290">Paul
        Duro </a>
      says that with the frame, “we experience
      the illusory coherence of the artwork,” and that it is “a marker of limits.”
    </p>
    <p>
      Another note about frames is that they often aren’t interpreted as being consistently on the
      same plane, with changes in depth along the border. It would be interesting to explore this
      further.
    </p>
    <div class='img-holder'>
      <img src='images/pheasant_with_frame_input.png'>
      <img src='images/pheasant_with_frame_output.png'>
      <img src='images/pheasant_no_frame_input.png'>
      <img src='images/pheasant_no_frame_output.png'>
    </div>
    <div class='caption'>Pheasant Shooting, by Henry Thomas Alken, 1820 (<a target="blank"
         href="https://artsandculture.google.com/asset/pheasant-shooting/fgG_sfeG64eMwA">source</a>)
    </div>
    <div class='img-holder'>
      <img src='images/winged_with_frame_input.png'>
      <img src='images/winged_with_frame_output.png'>
      <img src='images/winged_no_frame_input.png'>
      <img src='images/winged_no_frame_output.png'>
    </div>
    <div class='caption'>Winged Figure Seated Upon a Rock, by Abbott Handerson Thayer, 1914 (<a
         target="blank"
         href="https://artsandculture.google.com/asset/winged-figure-seated-upon-a-rock/BwHUF8iHPTRljQ">source</a>)
    </div>
    <h3>Faces</h3>
    <p>
      In portraiture, faces are often painted to be more dimensional and are brightly lit to stand
      out
      as a focal point, with a flatter, more muted rendering of surrounding areas. We found that
      in
      depth predictions for portraits, the face is often sharply brighter than the background and
      juts
      out of the canvas.
    </p>
    <p>
      Based on the skew toward Western art traditions in this data set, we additionally checked
      whether the effect seemed to hold for a wider variety of skin types. Indeed, as seen below,
      this
      seems to be the case.
    </p>
    <div class='img-holder'>
      <img src='images/portrait.png'>
      <img src='images/portrait_depth.png'>
    </div>
    <div class='caption'>My Little Brother, Jong Gu Lee, 1985
      <a target="blank"
         href="https://artsandculture.google.com/asset/JwH8qA4j8_Ji4w">source</a>)</div>

    <div class='img-holder'>
      <img src='images/portrait_2.png'>
      <img src='images/portrait_2_depth.png'>
    </div>
    <div class='caption'>Self-Portrait, Joshua Reynolds, 1745/1749
      <a target="blank"
         href="https://artsandculture.google.com/asset/_wHDJZ0TGlEIWw">source</a>)</div>

    <div class='img-holder'>
      <img src='images/baron.png'>
      <img src='images/baron depth.png'>
    </div>
    <div class='caption'>John, 1st Baron Byron, Kehinde Wiley, 2013, (<a target="blank"
         href="https://artsandculture.google.com/exhibit/MwLCMuU4yTWnLA">source</a>)</div>
    <h3>Looking at Depth across Different Styles
    </h3>
    <p>
      A natural question is whether there are trends in the model across time and styles. In the
      Western tradition, many of the depth techniques described earlier (namely linear perspective)
      matured around the Renaissance in the 15th and 16th centuries. Then, starting around the end
      of the 19th century, many Modern Art movements began to break from this tradition of
      portraying illusionistic depth in representational images and began to abstract their
      subjects. In addition, while the classical Western art tends to pursue illusionistic realism,
      some non-Western traditions of painting have long gravitated towards flatter colors and more
      stylized representations.
    </p>
    <div class='img-holder'>
      <img class='img-flat'
           src='images/venus.jpeg'>
      <img class='img-flat'
           src='images/print.jpeg'>
      <img class='img-flat'
           src='images/poodle.jpeg'>
    </div>
    <div class='caption'>
      A Dutch painting from the 1600s, (<a
         href='https://artsandculture.google.com/asset/jgFdAT2ytaDZ-w'
         target='_blank'>source</a>),
      a Chinese print from the 1900s (<a
         href='https://artsandculture.google.com/asset/dQEmAW0J0wjZ2g'
         target='_blank'>source</a>),
      and a French print from the 1900s (<a
         href='https://artsandculture.google.com/asset/KwFEmXRK_bbrtw'
         target='_blank'>source</a>)
    </div>
    <p>
      Given the evolution of depth depiction over time, we initially expected there might be an
      increasing trend around 1500, and a decreasing trend in the 1900s due to the development of
      perspective and the later modern art movements that flattened illusionistic paintings.
    </p>
    <p>
      But how do you measure this change? There are various metrics to approximate the overall depth
      of the image given by the model, that is, a scalar depth value for the image from the pixels
      of the depth map. We tried range (maximum depth - minimum depth), standard deviation, and
      median (a way to capture how much of the scene was “foreground” vs the default “background.”)
    </p>
    <p>
      We then calculated the mean and standard deviation of the distribution of these metrics for
      two
      groups of paintings:
    </p>
    <ul>
      <li>
        <b>Realistic / naturalistic:</b> Academic art, American Realism, Realism, Neoclassicism
      </li>
      <li>
        <b>Non-realistic / abstract:</b> Street art, Abstract art,
        Geometric
        abstraction, Shin-hanga, American modernism, Ukiyo-e
      </li>
    </ul>
    <p>
      Surprisingly, we found that there isn’t much statistical significance for these depth
      metrics
      across the two style groups.
    </p>
    <img src='images/boxplot.png'>
    <p>
      Similarly, plotting a number of art movements’ aggregate depth values over time, we found
      they
      also fall within a similar range. Note that the error bars are for standard deviation in the
      y
      direction, and year in the x direction, for a number of art movements,
    </p>
    <img src='images/art_styles_error_bars.png'>
    <p>
      The fact that depth doesn’t seem to be correlated with either time or artistic style raises
      an
      interesting question. Which is wrong, our interpretation of depth in art history, or the
      model’s
      interpretations of the images?
    </p>
    <p>
      Of course, all these metrics have a potential flaw: perhaps some depth image outputs are not
      just flat or deep, but actually incorrect. This, of course, is impossible to calculate as we
      have no ground truth of depth for these images. A future direction could be to try and find
      an
      uncertainty measure for the depth, and if this correlates more with art movements.
    </p>
    <h2>Conclusion</h2>
    <p>
      As with many machine learning models, depth prediction models can be hard to accurately
      evaluate and debug. However, there has been much less work on interpreting their failures than
      there has been on other image models. This raises some interesting questions. When they fail,
      how do they fail? And how, overall, are they making predictions?
    </p>
    <p>
      One lens through which to approach this is art history: humans have been attempting to define
      and control two dimensional depth representations for millennia, and perhaps we can take
      advantage of this. By observing how the model performs on images that are out of its training
      distribution in some ways (but not all), we can begin to isolate and explore what factors the
      model may be focusing on to make its prediction. We hope that this visualization interface in
      dataset can be helpful to other researchers in qualitatively exploring their models as well.
    </p>
    <h2>Thanks!</h2>
    <div style="color:#999;font-style:italic;padding-bottom:30px;">
      Thanks to Martin Wattenberg, Fernanda Viegas, Noah Snavely, Richard Tucker, D. Sculley, and
      Jasper Snoek for their help, feedback,
      and editing.
    </div>
</body>

</html>
